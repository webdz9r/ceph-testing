2022-03-04 18:04:10.651311 I | rookcmd: starting Rook v1.8.6 with arguments '/usr/local/bin/rook ceph operator'
2022-03-04 18:04:10.651423 I | rookcmd: flag values: --enable-machine-disruption-budget=false, --help=false, --kubeconfig=, --log-level=INFO, --operator-image=, --service-account=
2022-03-04 18:04:10.651426 I | cephcmd: starting Rook-Ceph operator
2022-03-04 18:04:10.961014 I | cephcmd: base ceph version inside the rook operator image is "ceph version 16.2.7 (dd0603118f56ab514f133c8d2e3adfc983942503) pacific (stable)"
2022-03-04 18:04:10.969172 I | op-k8sutil: ROOK_CURRENT_NAMESPACE_ONLY="false" (env var)
2022-03-04 18:04:10.969258 I | operator: watching all namespaces for Ceph CRs
2022-03-04 18:04:10.969327 I | operator: setting up schemes
2022-03-04 18:04:10.973298 I | operator: setting up the controller-runtime manager
2022-03-04 18:04:11.882004 I | operator: looking for admission webhook secret "rook-ceph-admission-controller"
2022-03-04 18:04:11.885307 I | operator: admission webhook secret "rook-ceph-admission-controller" not found. proceeding without the admission controller
2022-03-04 18:04:11.885351 I | ceph-cluster-controller: successfully started
2022-03-04 18:04:11.885404 I | ceph-cluster-controller: enabling hotplug orchestration
2022-03-04 18:04:11.885415 I | ceph-crashcollector-controller: successfully started
2022-03-04 18:04:11.885467 I | ceph-block-pool-controller: successfully started
2022-03-04 18:04:11.885500 I | ceph-object-store-user-controller: successfully started
2022-03-04 18:04:11.885526 I | ceph-object-realm-controller: successfully started
2022-03-04 18:04:11.885540 I | ceph-object-zonegroup-controller: successfully started
2022-03-04 18:04:11.885554 I | ceph-object-zone-controller: successfully started
2022-03-04 18:04:11.885668 I | ceph-object-controller: successfully started
2022-03-04 18:04:11.885705 I | ceph-file-controller: successfully started
2022-03-04 18:04:11.885734 I | ceph-nfs-controller: successfully started
2022-03-04 18:04:11.885759 I | ceph-rbd-mirror-controller: successfully started
2022-03-04 18:04:11.885791 I | ceph-client-controller: successfully started
2022-03-04 18:04:11.885810 I | ceph-filesystem-mirror-controller: successfully started
2022-03-04 18:04:11.885828 I | operator: rook-ceph-operator-config-controller successfully started
2022-03-04 18:04:11.885839 I | ceph-csi: rook-ceph-operator-csi-controller successfully started
2022-03-04 18:04:11.885851 I | op-bucket-prov: rook-ceph-operator-bucket-controller successfully started
2022-03-04 18:04:11.885864 I | ceph-bucket-topic: successfully started
2022-03-04 18:04:11.885876 I | ceph-bucket-notification: successfully started
2022-03-04 18:04:11.885884 I | ceph-bucket-notification: successfully started
2022-03-04 18:04:11.885895 I | ceph-fs-subvolumegroup-controller: successfully started
2022-03-04 18:04:11.887311 I | operator: starting the controller-runtime manager
2022-03-04 18:04:12.001032 I | op-k8sutil: ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS="15" (configmap)
2022-03-04 18:04:12.001068 I | op-k8sutil: ROOK_LOG_LEVEL="INFO" (configmap)
2022-03-04 18:04:12.001079 I | op-k8sutil: ROOK_ENABLE_DISCOVERY_DAEMON="false" (configmap)
2022-03-04 18:04:12.001786 I | ceph-csi: CSI Ceph RBD driver disabled
2022-03-04 18:04:12.001805 I | op-k8sutil: removing daemonset csi-rbdplugin if it exists
2022-03-04 18:04:12.004506 I | operator: rook-ceph-operator-config-controller done reconciling
2022-03-04 18:04:12.023268 I | op-k8sutil: Removed daemonset csi-rbdplugin
2022-03-04 18:04:12.037418 I | op-k8sutil: "csi-rbdplugin" still found. waiting...
2022-03-04 18:04:20.055715 I | op-k8sutil: confirmed csi-rbdplugin does not exist
2022-03-04 18:04:20.055738 I | op-k8sutil: removing deployment csi-rbdplugin-provisioner if it exists
2022-03-04 18:04:20.060238 I | op-k8sutil: Removed deployment csi-rbdplugin-provisioner
2022-03-04 18:04:20.103576 I | op-k8sutil: "csi-rbdplugin-provisioner" still found. waiting...
2022-03-04 18:04:30.133933 I | op-k8sutil: confirmed csi-rbdplugin-provisioner does not exist
2022-03-04 18:04:30.171006 I | ceph-csi: successfully removed CSI Ceph RBD driver
2022-03-04 18:04:30.171077 I | ceph-csi: CSI CephFS driver disabled
2022-03-04 18:04:30.171094 I | op-k8sutil: removing daemonset csi-cephfsplugin if it exists
2022-03-04 18:04:30.179835 I | op-k8sutil: Removed daemonset csi-cephfsplugin
2022-03-04 18:04:30.185096 I | op-k8sutil: "csi-cephfsplugin" still found. waiting...
2022-03-04 18:04:34.196431 I | op-k8sutil: confirmed csi-cephfsplugin does not exist
2022-03-04 18:04:34.196523 I | op-k8sutil: removing deployment csi-cephfsplugin-provisioner if it exists
2022-03-04 18:04:34.202431 I | op-k8sutil: Removed deployment csi-cephfsplugin-provisioner
2022-03-04 18:04:34.206818 I | op-k8sutil: "csi-cephfsplugin-provisioner" still found. waiting...
2022-03-04 18:04:40.228289 I | op-k8sutil: confirmed csi-cephfsplugin-provisioner does not exist
2022-03-04 18:04:40.305912 I | ceph-csi: successfully removed CSI CephFS driver
2022-03-04 18:04:57.471061 I | ceph-spec: adding finalizer "cephcluster.ceph.rook.io" on "rook-ceph"
2022-03-04 18:04:57.476035 I | clusterdisruption-controller: deleted all legacy node drain canary pods
2022-03-04 18:04:57.480824 I | ceph-cluster-controller: reconciling ceph cluster in namespace "rook-ceph"
2022-03-04 18:04:57.495955 I | ceph-cluster-controller: clusterInfo not yet found, must be a new cluster.
2022-03-04 18:04:57.524886 I | ceph-spec: detecting the ceph image version for image quay.io/ceph/ceph:v16.2.7...
2022-03-04 18:04:57.552352 I | ceph-csi: successfully created csi config map "rook-ceph-csi-config"
2022-03-04 18:04:57.557137 I | op-k8sutil: ROOK_CSI_ENABLE_RBD="true" (configmap)
2022-03-04 18:04:57.557158 I | op-k8sutil: ROOK_CSI_ENABLE_CEPHFS="true" (configmap)
2022-03-04 18:04:57.557163 I | op-k8sutil: ROOK_CSI_ALLOW_UNSUPPORTED_VERSION="false" (configmap)
2022-03-04 18:04:57.557166 I | op-k8sutil: ROOK_CSI_ENABLE_GRPC_METRICS="false" (configmap)
2022-03-04 18:04:57.557170 I | op-k8sutil: CSI_ENABLE_HOST_NETWORK="true" (default)
2022-03-04 18:04:57.557173 I | op-k8sutil: ROOK_CSI_CEPH_IMAGE="quay.io/cephcsi/cephcsi:v3.5.1" (default)
2022-03-04 18:04:57.557177 I | op-k8sutil: ROOK_CSI_REGISTRAR_IMAGE="k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.5.0" (default)
2022-03-04 18:04:57.557185 I | op-k8sutil: ROOK_CSI_PROVISIONER_IMAGE="k8s.gcr.io/sig-storage/csi-provisioner:v3.1.0" (default)
2022-03-04 18:04:57.557189 I | op-k8sutil: ROOK_CSI_ATTACHER_IMAGE="k8s.gcr.io/sig-storage/csi-attacher:v3.4.0" (default)
2022-03-04 18:04:57.557192 I | op-k8sutil: ROOK_CSI_SNAPSHOTTER_IMAGE="k8s.gcr.io/sig-storage/csi-snapshotter:v5.0.1" (default)
2022-03-04 18:04:57.557207 I | op-k8sutil: ROOK_CSI_KUBELET_DIR_PATH="/var/lib/kubelet" (default)
2022-03-04 18:04:57.557217 I | op-k8sutil: CSI_VOLUME_REPLICATION_IMAGE="quay.io/csiaddons/volumereplication-operator:v0.3.0" (default)
2022-03-04 18:04:57.557221 I | op-k8sutil: ROOK_CSIADDONS_IMAGE="quay.io/csiaddons/k8s-sidecar:v0.2.1" (default)
2022-03-04 18:04:57.557224 I | op-k8sutil: ROOK_CSI_CEPHFS_POD_LABELS="" (default)
2022-03-04 18:04:57.557227 I | op-k8sutil: ROOK_CSI_RBD_POD_LABELS="" (default)
2022-03-04 18:04:57.557231 I | ceph-csi: detecting the ceph csi image version for image "quay.io/cephcsi/cephcsi:v3.5.1"
2022-03-04 18:04:57.557313 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2022-03-04 18:04:57.557324 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="role=storage-node" (configmap)
2022-03-04 18:04:59.510370 I | ceph-csi: Detected ceph CSI image version: "v3.5.1"
2022-03-04 18:04:59.516895 I | op-k8sutil: CSI_FORCE_CEPHFS_KERNEL_CLIENT="true" (configmap)
2022-03-04 18:04:59.516917 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2022-03-04 18:04:59.516921 I | op-k8sutil: CSI_CEPHFS_GRPC_METRICS_PORT="9091" (default)
2022-03-04 18:04:59.516925 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2022-03-04 18:04:59.516928 I | op-k8sutil: CSI_CEPHFS_LIVENESS_METRICS_PORT="9081" (default)
2022-03-04 18:04:59.516932 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9093" (configmap)
2022-03-04 18:04:59.516935 I | op-k8sutil: CSI_RBD_GRPC_METRICS_PORT="9093" (configmap)
2022-03-04 18:04:59.516938 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2022-03-04 18:04:59.516941 I | op-k8sutil: CSIADDONS_PORT="9070" (default)
2022-03-04 18:04:59.516944 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2022-03-04 18:04:59.516948 I | op-k8sutil: CSI_RBD_LIVENESS_METRICS_PORT="9080" (default)
2022-03-04 18:04:59.516952 I | op-k8sutil: CSI_PLUGIN_PRIORITY_CLASSNAME="" (default)
2022-03-04 18:04:59.516956 I | op-k8sutil: CSI_PROVISIONER_PRIORITY_CLASSNAME="" (default)
2022-03-04 18:04:59.516960 I | op-k8sutil: CSI_ENABLE_OMAP_GENERATOR="false" (default)
2022-03-04 18:04:59.516966 I | op-k8sutil: CSI_ENABLE_RBD_SNAPSHOTTER="true" (configmap)
2022-03-04 18:04:59.516972 I | op-k8sutil: CSI_ENABLE_CEPHFS_SNAPSHOTTER="true" (configmap)
2022-03-04 18:04:59.516995 I | op-k8sutil: CSI_ENABLE_VOLUME_REPLICATION="false" (configmap)
2022-03-04 18:04:59.517000 I | op-k8sutil: CSI_ENABLE_CSIADDONS="false" (configmap)
2022-03-04 18:04:59.517005 I | op-k8sutil: CSI_CEPHFS_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2022-03-04 18:04:59.517009 I | op-k8sutil: CSI_RBD_PLUGIN_UPDATE_STRATEGY="RollingUpdate" (default)
2022-03-04 18:04:59.517014 I | op-k8sutil: CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT="false" (configmap)
2022-03-04 18:04:59.517018 I | ceph-csi: Kubernetes version is 1.21
2022-03-04 18:04:59.517023 I | op-k8sutil: ROOK_CSI_RESIZER_IMAGE="k8s.gcr.io/sig-storage/csi-resizer:v1.4.0" (default)
2022-03-04 18:04:59.517027 I | op-k8sutil: CSI_LOG_LEVEL="" (default)
2022-03-04 18:04:59.536752 I | op-k8sutil: CSI_PROVISIONER_REPLICAS="2" (configmap)
2022-03-04 18:04:59.542663 I | op-k8sutil: CSI_PROVISIONER_TOLERATIONS="" (default)
2022-03-04 18:04:59.542686 I | op-k8sutil: CSI_PROVISIONER_NODE_AFFINITY="role=storage-node" (configmap)
2022-03-04 18:04:59.542699 I | op-k8sutil: CSI_PLUGIN_TOLERATIONS="" (default)
2022-03-04 18:04:59.542702 I | op-k8sutil: CSI_PLUGIN_NODE_AFFINITY="role=storage-node" (configmap)
2022-03-04 18:04:59.542707 I | op-k8sutil: CSI_RBD_PLUGIN_TOLERATIONS="" (default)
2022-03-04 18:04:59.542710 I | op-k8sutil: CSI_RBD_PLUGIN_NODE_AFFINITY="" (default)
2022-03-04 18:04:59.542713 I | op-k8sutil: CSI_RBD_PLUGIN_RESOURCE="" (default)
2022-03-04 18:04:59.555866 I | op-k8sutil: CSI_RBD_PROVISIONER_TOLERATIONS="" (default)
2022-03-04 18:04:59.555903 I | op-k8sutil: CSI_RBD_PROVISIONER_NODE_AFFINITY="" (default)
2022-03-04 18:04:59.555908 I | op-k8sutil: CSI_RBD_PROVISIONER_RESOURCE="" (default)
2022-03-04 18:04:59.598415 I | ceph-csi: successfully started CSI Ceph RBD driver
2022-03-04 18:04:59.628207 I | op-k8sutil: CSI_CEPHFS_PLUGIN_TOLERATIONS="" (default)
2022-03-04 18:04:59.628229 I | op-k8sutil: CSI_CEPHFS_PLUGIN_NODE_AFFINITY="" (default)
2022-03-04 18:04:59.628233 I | op-k8sutil: CSI_CEPHFS_PLUGIN_RESOURCE="" (default)
2022-03-04 18:04:59.647688 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_TOLERATIONS="" (default)
2022-03-04 18:04:59.647887 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_NODE_AFFINITY="" (default)
2022-03-04 18:04:59.647947 I | op-k8sutil: CSI_CEPHFS_PROVISIONER_RESOURCE="" (default)
2022-03-04 18:04:59.679495 I | ceph-csi: successfully started CSI CephFS driver
2022-03-04 18:04:59.732323 I | op-k8sutil: CSI_RBD_FSGROUPPOLICY="ReadWriteOnceWithFSType" (configmap)
2022-03-04 18:04:59.769768 I | ceph-csi: CSIDriver object created for driver "rook-ceph.rbd.csi.ceph.com"
2022-03-04 18:04:59.769799 I | op-k8sutil: CSI_CEPHFS_FSGROUPPOLICY="None" (configmap)
2022-03-04 18:04:59.791290 I | ceph-csi: CSIDriver object created for driver "rook-ceph.cephfs.csi.ceph.com"
2022-03-04 18:05:03.887546 I | ceph-spec: detected ceph image version: "16.2.7-0 pacific"
2022-03-04 18:05:03.887576 I | ceph-cluster-controller: validating ceph version from provided image
2022-03-04 18:05:03.892832 I | ceph-cluster-controller: cluster "rook-ceph": version "16.2.7-0 pacific" detected for image "quay.io/ceph/ceph:v16.2.7"
2022-03-04 18:05:03.944073 I | op-mon: start running mons
2022-03-04 18:05:04.034784 I | op-mon: creating mon secrets for a new cluster
2022-03-04 18:05:04.064843 I | op-mon: existing maxMonID not found or failed to load. configmaps "rook-ceph-mon-endpoints" not found
2022-03-04 18:05:04.077000 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":[]}] data: mapping:{"node":{}} maxMonId:-1]
2022-03-04 18:05:04.877420 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2022-03-04 18:05:04.877633 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2022-03-04 18:05:06.477056 I | op-mon: targeting the mon count 3
2022-03-04 18:05:06.678958 I | op-mon: created canary monitor rook-ceph-mon-a-canary pvc rook-ceph-mon-a
2022-03-04 18:05:06.692825 I | op-mon: created canary deployment rook-ceph-mon-a-canary
2022-03-04 18:05:06.884955 I | op-mon: created canary monitor rook-ceph-mon-b-canary pvc rook-ceph-mon-b
2022-03-04 18:05:06.891255 I | op-mon: created canary deployment rook-ceph-mon-b-canary
2022-03-04 18:05:07.479143 I | op-mon: created canary monitor rook-ceph-mon-c-canary pvc rook-ceph-mon-c
2022-03-04 18:05:07.493193 I | op-mon: created canary deployment rook-ceph-mon-c-canary
2022-03-04 18:05:08.077072 I | op-mon: parsing mon endpoints: 
2022-03-04 18:05:08.077098 W | op-mon: ignoring invalid monitor 
2022-03-04 18:05:08.077133 I | op-k8sutil: ROOK_OBC_WATCH_OPERATOR_NAMESPACE="true" (configmap)
2022-03-04 18:05:08.077140 I | op-bucket-prov: ceph bucket provisioner launched watching for provisioner "rook-ceph.ceph.rook.io/bucket"
2022-03-04 18:05:08.078360 I | op-bucket-prov: successfully reconciled bucket provisioner
I0304 18:05:08.078470       1 manager.go:135] objectbucket.io/provisioner-manager "msg"="starting provisioner"  "name"="rook-ceph.ceph.rook.io/bucket"
2022-03-04 18:05:12.094105 I | op-mon: canary monitor deployment rook-ceph-mon-a-canary scheduled to wbp-premium-u1tii
2022-03-04 18:05:12.094133 I | op-mon: mon "a" placement using native scheduler
2022-03-04 18:05:12.290199 I | op-mon: canary monitor deployment rook-ceph-mon-b-canary scheduled to wbp-premium-u1ti4
2022-03-04 18:05:12.290226 I | op-mon: mon "b" placement using native scheduler
2022-03-04 18:05:17.898784 I | op-mon: canary monitor deployment rook-ceph-mon-c-canary scheduled to wbp-premium-u1tih
2022-03-04 18:05:17.898803 I | op-mon: mon "c" placement using native scheduler
2022-03-04 18:05:17.903796 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-a-canary"
2022-03-04 18:05:17.917958 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-b-canary"
2022-03-04 18:05:17.959181 I | op-mon: cleaning up canary monitor deployment "rook-ceph-mon-c-canary"
2022-03-04 18:05:18.034515 I | op-mon: creating mon a
2022-03-04 18:05:18.083169 I | op-mon: mon "a" endpoint is [v2:10.245.193.141:3300,v1:10.245.193.141:6789]
2022-03-04 18:05:18.194647 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.193.141:6789"]}] data:a=10.245.193.141:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:-1]
2022-03-04 18:05:18.194886 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2022-03-04 18:05:18.194947 I | op-mon: monitor endpoints changed, updating the bootstrap peer token
2022-03-04 18:05:18.252987 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2022-03-04 18:05:18.253258 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2022-03-04 18:05:18.276073 I | op-mon: 0 of 1 expected mons are ready. creating or updating deployments without checking quorum in attempt to achieve a healthy mon cluster
2022-03-04 18:05:18.500441 I | op-mon: updating maxMonID from -1 to 0 after committing mon "a"
2022-03-04 18:05:19.291100 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.193.141:6789"]}] data:a=10.245.193.141:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:0]
2022-03-04 18:05:19.291123 I | op-mon: waiting for mon quorum with [a]
2022-03-04 18:05:19.494090 I | op-mon: mon a is not yet running
2022-03-04 18:05:19.494323 I | op-mon: mons running: []
2022-03-04 18:05:39.671504 I | op-mon: mons running: [a]
2022-03-04 18:05:49.030221 I | op-mon: Monitors in quorum: [a]
2022-03-04 18:05:49.030246 I | op-mon: mons created: 1
2022-03-04 18:05:49.448933 I | op-mon: waiting for mon quorum with [a]
2022-03-04 18:05:49.455345 I | op-mon: mons running: [a]
2022-03-04 18:05:49.873336 I | op-mon: Monitors in quorum: [a]
2022-03-04 18:05:49.873363 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:05:50.349110 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:05:50.349137 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:05:50.835850 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:05:50.835964 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:05:51.277317 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:05:51.277395 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:05:51.771866 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:05:51.771903 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:05:52.231779 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:05:52.231805 I | op-config: deleting "log file" option from the mon configuration database
2022-03-04 18:05:52.667883 I | op-config: successfully deleted "log file" option from the mon configuration database
2022-03-04 18:05:52.667913 I | op-mon: creating mon b
2022-03-04 18:05:52.723413 I | op-mon: mon "a" endpoint is [v2:10.245.193.141:3300,v1:10.245.193.141:6789]
2022-03-04 18:05:52.739209 I | op-mon: mon "b" endpoint is [v2:10.245.78.198:3300,v1:10.245.78.198:6789]
2022-03-04 18:05:52.763538 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.78.198:6789","10.245.193.141:6789"]}] data:a=10.245.193.141:6789,b=10.245.78.198:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:0]
2022-03-04 18:05:52.872939 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2022-03-04 18:05:52.873093 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2022-03-04 18:05:53.279273 I | op-mon: 1 of 2 expected mon deployments exist. creating new deployment(s).
2022-03-04 18:05:53.471658 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2022-03-04 18:05:53.486809 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2022-03-04 18:05:53.871947 I | op-mon: updating maxMonID from 0 to 1 after committing mon "b"
2022-03-04 18:05:54.672492 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.193.141:6789","10.245.78.198:6789"]}] data:a=10.245.193.141:6789,b=10.245.78.198:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:1]
2022-03-04 18:05:54.672597 I | op-mon: waiting for mon quorum with [a b]
2022-03-04 18:05:55.073673 I | op-mon: mon b is not yet running
2022-03-04 18:05:55.073850 I | op-mon: mons running: [a]
2022-03-04 18:05:55.437365 I | op-mon: Monitors in quorum: [a]
2022-03-04 18:05:55.437443 I | op-mon: mons created: 2
2022-03-04 18:05:55.935303 I | op-mon: waiting for mon quorum with [a b]
2022-03-04 18:05:55.959428 I | op-mon: mon b is not yet running
2022-03-04 18:05:55.959456 I | op-mon: mons running: [a]
2022-03-04 18:06:00.971543 I | op-mon: mon b is not yet running
2022-03-04 18:06:00.971579 I | op-mon: mons running: [a]
2022-03-04 18:06:05.984132 I | op-mon: mon b is not yet running
2022-03-04 18:06:05.984176 I | op-mon: mons running: [a]
2022-03-04 18:06:10.995606 I | op-mon: mon b is not yet running
2022-03-04 18:06:10.995632 I | op-mon: mons running: [a]
2022-03-04 18:06:16.018673 I | op-mon: mon b is not yet running
2022-03-04 18:06:16.018699 I | op-mon: mons running: [a]
2022-03-04 18:06:21.029320 I | op-mon: mon b is not yet running
2022-03-04 18:06:21.029344 I | op-mon: mons running: [a]
2022-03-04 18:06:26.041571 I | op-mon: mon b is not yet running
2022-03-04 18:06:26.041597 I | op-mon: mons running: [a]
2022-03-04 18:06:31.054358 I | op-mon: mons running: [a b]
2022-03-04 18:06:35.874818 I | op-mon: Monitors in quorum: [a b]
2022-03-04 18:06:35.874850 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:06:36.340880 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:06:36.340908 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:06:36.833163 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:06:36.833195 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:06:37.262960 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:06:37.262984 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:06:37.739751 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:06:37.739780 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:06:38.163822 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:06:38.163850 I | op-config: deleting "log file" option from the mon configuration database
2022-03-04 18:06:38.654853 I | op-config: successfully deleted "log file" option from the mon configuration database
2022-03-04 18:06:38.654887 I | op-mon: creating mon c
2022-03-04 18:06:38.697796 I | op-mon: mon "a" endpoint is [v2:10.245.193.141:3300,v1:10.245.193.141:6789]
2022-03-04 18:06:38.757898 I | op-mon: mon "b" endpoint is [v2:10.245.78.198:3300,v1:10.245.78.198:6789]
2022-03-04 18:06:38.783261 I | op-mon: mon "c" endpoint is [v2:10.245.185.46:3300,v1:10.245.185.46:6789]
2022-03-04 18:06:39.112754 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.78.198:6789","10.245.185.46:6789","10.245.193.141:6789"]}] data:a=10.245.193.141:6789,b=10.245.78.198:6789,c=10.245.185.46:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:1]
2022-03-04 18:06:39.659475 I | cephclient: writing config file /var/lib/rook/rook-ceph/rook-ceph.config
2022-03-04 18:06:39.659650 I | cephclient: generated admin config in /var/lib/rook/rook-ceph
2022-03-04 18:06:40.068110 I | op-mon: 2 of 3 expected mon deployments exist. creating new deployment(s).
2022-03-04 18:06:40.259710 I | op-mon: deployment for mon rook-ceph-mon-a already exists. updating if needed
2022-03-04 18:06:40.270138 I | op-k8sutil: deployment "rook-ceph-mon-a" did not change, nothing to update
2022-03-04 18:06:40.459062 I | op-mon: deployment for mon rook-ceph-mon-b already exists. updating if needed
2022-03-04 18:06:40.468627 I | op-k8sutil: deployment "rook-ceph-mon-b" did not change, nothing to update
2022-03-04 18:06:40.858949 I | op-mon: updating maxMonID from 1 to 2 after committing mon "c"
2022-03-04 18:06:41.661153 I | op-mon: saved mon endpoints to config map map[csi-cluster-config-json:[{"clusterID":"rook-ceph","monitors":["10.245.78.198:6789","10.245.185.46:6789","10.245.193.141:6789"]}] data:a=10.245.193.141:6789,b=10.245.78.198:6789,c=10.245.185.46:6789 mapping:{"node":{"a":null,"b":null,"c":null}} maxMonId:2]
2022-03-04 18:06:41.661261 I | op-mon: waiting for mon quorum with [a b c]
2022-03-04 18:06:42.261088 I | op-mon: mon c is not yet running
2022-03-04 18:06:42.261128 I | op-mon: mons running: [a b]
2022-03-04 18:06:42.653709 I | op-mon: Monitors in quorum: [a b]
2022-03-04 18:06:42.653737 I | op-mon: mons created: 3
2022-03-04 18:06:43.133175 I | op-mon: waiting for mon quorum with [a b c]
2022-03-04 18:06:43.153883 I | op-mon: mon c is not yet running
2022-03-04 18:06:43.153910 I | op-mon: mons running: [a b]
2022-03-04 18:06:48.180072 I | op-mon: mon c is not yet running
2022-03-04 18:06:48.180098 I | op-mon: mons running: [a b]
2022-03-04 18:06:53.197572 I | op-mon: mon c is not yet running
2022-03-04 18:06:53.197610 I | op-mon: mons running: [a b]
2022-03-04 18:06:58.215817 I | op-mon: mon c is not yet running
2022-03-04 18:06:58.215842 I | op-mon: mons running: [a b]
2022-03-04 18:07:03.234226 I | op-mon: mon c is not yet running
2022-03-04 18:07:03.234269 I | op-mon: mons running: [a b]
2022-03-04 18:07:08.250877 I | op-mon: mon c is not yet running
2022-03-04 18:07:08.251087 I | op-mon: mons running: [a b]
2022-03-04 18:07:13.267918 I | op-mon: mon c is not yet running
2022-03-04 18:07:13.267952 I | op-mon: mons running: [a b]
2022-03-04 18:07:18.284007 I | op-mon: mon c is not yet running
2022-03-04 18:07:18.284032 I | op-mon: mons running: [a b]
2022-03-04 18:07:23.301554 I | op-mon: mons running: [a b c]
2022-03-04 18:07:24.232848 W | op-mon: monitor c is not in quorum list
2022-03-04 18:07:29.249246 I | op-mon: mons running: [a b c]
2022-03-04 18:07:29.653017 I | op-mon: Monitors in quorum: [a b c]
2022-03-04 18:07:29.653059 I | op-config: setting "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:07:30.138770 I | op-config: successfully set "global"="mon allow pool delete"="true" option to the mon configuration database
2022-03-04 18:07:30.138798 I | op-config: setting "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:07:30.559653 I | op-config: successfully set "global"="mon cluster log file"="" option to the mon configuration database
2022-03-04 18:07:30.559682 I | op-config: setting "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:07:31.034354 I | op-config: successfully set "global"="mon allow pool size one"="true" option to the mon configuration database
2022-03-04 18:07:31.034378 I | op-config: setting "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:07:31.473526 I | op-config: successfully set "global"="osd scrub auto repair"="true" option to the mon configuration database
2022-03-04 18:07:31.473556 I | op-config: setting "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:07:31.953262 I | op-config: successfully set "global"="log to file"="false" option to the mon configuration database
2022-03-04 18:07:31.953291 I | op-config: deleting "log file" option from the mon configuration database
2022-03-04 18:07:32.426690 I | op-config: successfully deleted "log file" option from the mon configuration database
2022-03-04 18:07:32.442799 I | op-mon: checking for orphaned mon resources
2022-03-04 18:07:32.467350 I | cephclient: getting or creating ceph auth key "client.csi-rbd-provisioner"
2022-03-04 18:07:32.929126 I | cephclient: getting or creating ceph auth key "client.csi-rbd-node"
2022-03-04 18:07:33.455474 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-provisioner"
2022-03-04 18:07:33.931308 I | cephclient: getting or creating ceph auth key "client.csi-cephfs-node"
2022-03-04 18:07:34.608818 I | ceph-csi: created kubernetes csi secrets for cluster "rook-ceph"
2022-03-04 18:07:34.608839 I | cephclient: getting or creating ceph auth key "client.crash"
2022-03-04 18:07:35.228070 I | ceph-crashcollector-controller: created kubernetes crash collector secret for cluster "rook-ceph"
2022-03-04 18:07:35.670281 I | cephclient: successfully enabled msgr2 protocol
2022-03-04 18:07:35.670316 I | op-config: deleting "mon_mds_skip_sanity" option from the mon configuration database
2022-03-04 18:07:36.229479 I | op-config: successfully deleted "mon_mds_skip_sanity" option from the mon configuration database
2022-03-04 18:07:36.229506 I | cephclient: create rbd-mirror bootstrap peer token "client.rbd-mirror-peer"
2022-03-04 18:07:36.229510 I | cephclient: getting or creating ceph auth key "client.rbd-mirror-peer"
2022-03-04 18:07:36.717668 I | cephclient: successfully created rbd-mirror bootstrap peer token for cluster "rook-ceph"
2022-03-04 18:07:36.731103 I | op-mgr: start running mgr
2022-03-04 18:07:36.731135 I | cephclient: getting or creating ceph auth key "mgr.a"
2022-03-04 18:07:37.401029 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-wbp-premium-u1tii": the object has been modified; please apply your changes to the latest version and try again
2022-03-04 18:07:48.427610 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-wbp-nodes-uh4bg": the object has been modified; please apply your changes to the latest version and try again
2022-03-04 18:07:58.243132 I | op-k8sutil: finished waiting for updated deployment "rook-ceph-mgr-a"
2022-03-04 18:07:58.247083 I | op-mgr: setting services to point to mgr "a"
2022-03-04 18:07:58.288851 I | op-mgr: no need to update service "rook-ceph-mgr"
2022-03-04 18:07:58.288877 I | op-mgr: no need to update service "rook-ceph-mgr-dashboard"
2022-03-04 18:07:58.289200 I | op-mgr: successful modules: balancer
2022-03-04 18:07:58.297927 I | op-osd: start running osds in namespace "rook-ceph"
2022-03-04 18:07:58.297948 I | op-osd: wait timeout for healthy OSDs during upgrade or restart is "10m0s"
2022-03-04 18:07:58.307863 I | op-osd: start provisioning the OSDs on PVCs, if needed
2022-03-04 18:07:58.323152 I | op-osd: creating 3 new PVCs for device set "set1"
2022-03-04 18:07:58.327023 I | op-osd: successfully provisioned PVC "set1-data-077d7s"
2022-03-04 18:07:58.331102 I | op-osd: successfully provisioned PVC "set1-data-1h5mll"
2022-03-04 18:07:58.342096 I | op-osd: successfully provisioned PVC "set1-data-2lh5pp"
2022-03-04 18:07:58.423129 I | op-osd: OSD will have its main bluestore block on "set1-data-077d7s"
2022-03-04 18:07:58.440123 I | op-osd: started OSD provisioning job for PVC "set1-data-077d7s"
2022-03-04 18:07:58.440148 I | op-osd: OSD will have its main bluestore block on "set1-data-1h5mll"
2022-03-04 18:07:58.624456 I | op-osd: started OSD provisioning job for PVC "set1-data-1h5mll"
2022-03-04 18:07:58.624666 I | op-osd: OSD will have its main bluestore block on "set1-data-2lh5pp"
2022-03-04 18:07:58.923972 I | op-osd: started OSD provisioning job for PVC "set1-data-2lh5pp"
2022-03-04 18:07:58.923993 I | op-osd: start provisioning the OSDs on nodes, if needed
2022-03-04 18:07:58.923997 I | op-osd: no nodes are defined for configuring OSDs on raw devices
2022-03-04 18:07:59.051871 I | op-osd: OSD orchestration status for PVC set1-data-077d7s is "starting"
2022-03-04 18:07:59.051895 I | op-osd: OSD orchestration status for PVC set1-data-1h5mll is "starting"
2022-03-04 18:07:59.051901 I | op-osd: OSD orchestration status for PVC set1-data-2lh5pp is "starting"
2022-03-04 18:08:00.727466 I | op-mgr: successful modules: prometheus
2022-03-04 18:08:01.744794 I | op-config: setting "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2022-03-04 18:08:02.497255 I | op-config: successfully set "global"="osd_pool_default_pg_autoscale_mode"="on" option to the mon configuration database
2022-03-04 18:08:02.497342 I | op-config: setting "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2022-03-04 18:08:03.319016 I | op-config: successfully set "global"="mon_pg_warn_min_per_osd"="0" option to the mon configuration database
2022-03-04 18:08:03.319129 I | op-mgr: successful modules: mgr module(s) from the spec
2022-03-04 18:08:05.733311 I | op-mgr: setting ceph dashboard "admin" login creds
2022-03-04 18:08:09.777966 I | op-mgr: successfully set ceph dashboard creds
2022-03-04 18:08:11.053267 I | op-config: setting "mgr.a"="mgr/dashboard/ssl"="false" option to the mon configuration database
2022-03-04 18:08:11.781624 I | op-config: successfully set "mgr.a"="mgr/dashboard/ssl"="false" option to the mon configuration database
2022-03-04 18:08:12.428356 I | op-config: setting "mgr.a"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2022-03-04 18:08:13.199088 I | op-config: successfully set "mgr.a"="mgr/dashboard/server_port"="7000" option to the mon configuration database
2022-03-04 18:08:13.199119 I | op-mgr: dashboard config has changed. restarting the dashboard module
2022-03-04 18:08:13.199124 I | op-mgr: restarting the mgr module
2022-03-04 18:08:15.510810 I | op-mgr: successful modules: dashboard
2022-03-04 18:08:20.559347 I | op-osd: OSD orchestration status for node set1-data-1h5mll is "orchestrating"
2022-03-04 18:08:20.986099 I | op-osd: OSD orchestration status for PVC set1-data-1h5mll is "orchestrating"
2022-03-04 18:08:21.513394 I | op-osd: OSD orchestration status for node set1-data-077d7s is "orchestrating"
2022-03-04 18:08:21.925323 I | op-osd: OSD orchestration status for PVC set1-data-077d7s is "orchestrating"
2022-03-04 18:08:27.508514 I | op-osd: OSD orchestration status for node set1-data-2lh5pp is "orchestrating"
2022-03-04 18:08:27.612910 I | op-osd: OSD orchestration status for PVC set1-data-1h5mll is "completed"
2022-03-04 18:08:27.612932 I | op-osd: creating OSD 0 on PVC "set1-data-1h5mll"
2022-03-04 18:08:27.612938 I | op-osd: OSD will have its main bluestore block on "set1-data-1h5mll"
2022-03-04 18:08:28.018816 I | op-osd: OSD orchestration status for PVC set1-data-2lh5pp is "orchestrating"
2022-03-04 18:08:28.359994 I | op-osd: OSD orchestration status for PVC set1-data-077d7s is "completed"
2022-03-04 18:08:28.360023 I | op-osd: creating OSD 1 on PVC "set1-data-077d7s"
2022-03-04 18:08:28.360031 I | op-osd: OSD will have its main bluestore block on "set1-data-077d7s"
2022-03-04 18:08:28.630966 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-wbp-premium-u1tii": the object has been modified; please apply your changes to the latest version and try again
2022-03-04 18:08:28.796075 E | ceph-crashcollector-controller: node reconcile failed on op "unchanged": Operation cannot be fulfilled on deployments.apps "rook-ceph-crashcollector-wbp-premium-u1tii": the object has been modified; please apply your changes to the latest version and try again
2022-03-04 18:08:36.011105 I | op-osd: OSD orchestration status for PVC set1-data-2lh5pp is "completed"
2022-03-04 18:08:36.011142 I | op-osd: creating OSD 2 on PVC "set1-data-2lh5pp"
2022-03-04 18:08:36.011201 I | op-osd: OSD will have its main bluestore block on "set1-data-2lh5pp"
2022-03-04 18:08:36.937867 I | op-osd: finished running OSDs in namespace "rook-ceph"
2022-03-04 18:08:36.937891 I | ceph-cluster-controller: done reconciling ceph cluster in namespace "rook-ceph"
2022-03-04 18:08:36.995963 I | ceph-cluster-controller: enabling ceph mon monitoring goroutine for cluster "rook-ceph"
2022-03-04 18:08:36.995988 I | ceph-cluster-controller: enabling ceph osd monitoring goroutine for cluster "rook-ceph"
2022-03-04 18:08:36.995995 I | ceph-cluster-controller: enabling ceph status monitoring goroutine for cluster "rook-ceph"
2022-03-04 18:08:38.764384 I | ceph-cluster-controller: Disabling the insecure global ID as no legacy clients are currently connected. If you still require the insecure connections, see the CVE to suppress the health warning and re-enable the insecure connections. https://docs.ceph.com/en/latest/security/CVE-2021-20288/
2022-03-04 18:08:38.764484 I | op-config: setting "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2022-03-04 18:08:39.457619 I | op-config: successfully set "mon"="auth_allow_insecure_global_id_reclaim"="false" option to the mon configuration database
2022-03-04 18:08:39.457825 I | ceph-cluster-controller: insecure global ID is now disabled
2022-03-04 18:09:22.837478 I | op-mon: checking if multiple mons are on the same node
